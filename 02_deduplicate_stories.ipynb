{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Story Deduplication\n",
    "\n",
    "This notebook processes multiple JSON files containing story data, removes duplicates based on story IDs, and exports clean deduplicated stories to a single CSV file for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FIELDS_TO_EXTRACT = [\n",
    "    \"story_id\",\n",
    "    \"title\",\n",
    "    \"story_text\",\n",
    "    \"created_at_i\",\n",
    "]\n",
    "\n",
    "INPUT_DIRECTORY = \"data/stories\"\n",
    "OUTPUT_CSV_PATH = \"data/stories_deduplicated.csv\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Implement core utilities for JSON loading, story extraction with deduplication logic, HTML-to-text conversion, and CSV export functionality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "\n",
    "\n",
    "def load_json_file(file_path: str | Path) -> dict | list | None:\n",
    "    \"\"\"Loads JSON data from a single file.\"\"\"\n",
    "    try:\n",
    "        return json.loads(Path(file_path).read_text(encoding=\"utf-8\"))\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_stories(seen_story_ids: set[str], stories_to_process: list[dict], fields_to_extract: list[str]) -> tuple[\n",
    "    set[str], list[dict]]:\n",
    "    \"\"\"Extract stories with deduplication\"\"\"\n",
    "    batch_extracted_stories: list[dict] = []\n",
    "    batch_seen_story_ids: set[str] = set()\n",
    "\n",
    "    if not isinstance(stories_to_process, list):\n",
    "        print(f\"Warning: Expected a list of stories, but got {type(stories_to_process)}. Skipping.\")\n",
    "        return batch_seen_story_ids, batch_extracted_stories\n",
    "\n",
    "    for story in stories_to_process:\n",
    "        story_id = story.get(\"story_id\")\n",
    "\n",
    "        if story_id in seen_story_ids or story_id in batch_seen_story_ids:\n",
    "            continue\n",
    "\n",
    "        batch_seen_story_ids.add(story_id)\n",
    "        extracted_story_data: dict = {}\n",
    "\n",
    "        for field in fields_to_extract:\n",
    "            value = story.get(field, \"\")\n",
    "            if field == \"story_text\" and value:\n",
    "                value = convert_html_to_plain_text(value)\n",
    "            extracted_story_data[field] = value\n",
    "\n",
    "        batch_extracted_stories.append(extracted_story_data)\n",
    "\n",
    "    return batch_seen_story_ids, batch_extracted_stories\n",
    "\n",
    "\n",
    "def convert_html_to_plain_text(s: str) -> str:\n",
    "    \"\"\"Convert HTML to plain text\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress false positives when story text contains URLs or file-like patterns\n",
    "        warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "        soup = BeautifulSoup(s, \"html.parser\")\n",
    "\n",
    "    text = soup.get_text()\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def save_to_csv(data: list[dict], csv_file_path: str | Path, fieldnames_list: list[str]) -> None:\n",
    "    \"\"\"Saves the extracted data to a CSV file.\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    Path(csv_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames_list)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)  # Use writerows for efficiency\n",
    "\n",
    "    print(f\"Successfully saved {len(data)} stories to {csv_file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Execute the main pipeline that discovers JSON files, processes each file while tracking global duplicates, and accumulates unique stories across all sources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "json_file_paths = glob.glob(os.path.join(INPUT_DIRECTORY, \"*.json\"))\n",
    "\n",
    "if not json_file_paths:\n",
    "    print(f\"No JSON files found in the directory: {INPUT_DIRECTORY}\")\n",
    "else:\n",
    "    print(f\"Found JSON files to process in '{INPUT_DIRECTORY}': {json_file_paths}\")\n",
    "\n",
    "all_extracted_stories = []\n",
    "seen_story_ids_global = set()  # To track uniqueness across all files globally\n",
    "total_stories_processed = 0  # New: count all stories processed (before deduplication)\n",
    "\n",
    "for file_path in json_file_paths:\n",
    "    print(f\"\\nProcessing file: {file_path}...\")\n",
    "    stories_data_from_file = load_json_file(file_path)\n",
    "\n",
    "    if stories_data_from_file:\n",
    "        # Count all stories in this file (before deduplication)\n",
    "        if isinstance(stories_data_from_file, list):\n",
    "            total_stories_processed += len(stories_data_from_file)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Expected a list of stories from {file_path}, got {type(stories_data_from_file)}\"\n",
    "            )\n",
    "\n",
    "        newly_seen_ids, extracted_stories = extract_stories(\n",
    "            seen_story_ids_global, stories_data_from_file, FIELDS_TO_EXTRACT\n",
    "        )\n",
    "        seen_story_ids_global.update(newly_seen_ids)\n",
    "        all_extracted_stories.extend(extracted_stories)\n",
    "        print(\n",
    "            f\"Finished processing {file_path}. Current total stories: {len(all_extracted_stories)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No data loaded from {file_path} or file was empty/corrupt.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Export & Report"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if all_extracted_stories:\n",
    "    save_to_csv(all_extracted_stories, OUTPUT_CSV_PATH, FIELDS_TO_EXTRACT)\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Total stories extracted (before deduplication): {total_stories_processed}\")\n",
    "    print(f\"Total unique stories after deduplication: {len(all_extracted_stories)}\")\n",
    "    print(\n",
    "        f\"Number of duplicates removed: {total_stories_processed - len(all_extracted_stories)}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo stories were extracted after processing all files.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
