{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Keyword Extraction\n",
    "\n",
    "This notebook uses an LLM to extract domain-specific keywords (LLM and DEV related) from story titles and content, with resumable processing to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "input_file = Path(\"data/stories_deduplicated.csv\")\n",
    "output_file = Path(\"data/stories_deduplicated_meta.csv\")\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Define the core utilities for LLM-based keyword extraction, progress tracking, CSV I/O, and error handling with resumable processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from llm import Agent\n",
    "import json\n",
    "import csv\n",
    "from typing import Set\n",
    "\n",
    "\n",
    "def initialize_output_file(output_file: Path) -> None:\n",
    "    \"\"\"Create output file with headers if it doesn't exist\"\"\"\n",
    "    if not output_file.exists():\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"story_id\", \"llm_keywords\", \"dev_keywords\"])\n",
    "        print(f\"Created new output file: {output_file}\")\n",
    "\n",
    "\n",
    "def get_processed_story_ids(output_file: Path) -> Set[str]:\n",
    "    \"\"\"Get set of already processed story IDs\"\"\"\n",
    "    if output_file.exists():\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        processed_ids = set(existing_df[\"story_id\"].tolist())\n",
    "        print(f\"Found {len(processed_ids)} already processed stories\")\n",
    "        return processed_ids\n",
    "    return set()\n",
    "\n",
    "\n",
    "def process_single_story(row: pd.Series, output_file: Path) -> bool:\n",
    "    \"\"\"Process a single story and save its keywords. Returns True if successful.\"\"\"\n",
    "    story_id = row[\"story_id\"]\n",
    "    title = str(row.get(\"title\", \"\"))\n",
    "    story_text = str(row.get(\"story_text\", \"\"))\n",
    "\n",
    "    try:\n",
    "        keywords = extract_keywords(title, story_text)\n",
    "        save_story_keywords(output_file, story_id, keywords)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing story {story_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_keywords(title: str, content: str) -> dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Extract LLM and DEV keywords from title and story text.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 'llm' and 'dev' keys containing sets of keywords\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = Path(\"03_system_prompt.md\").read_text()\n",
    "    INPUT = f\"\"\"\n",
    "    <story>\n",
    "      <title>{title}</title>\n",
    "      <content>{content}</content>\n",
    "    </story>\n",
    "    \"\"\"\n",
    "\n",
    "    response = (\n",
    "        Agent(\"deepseek-v3-0324:free\", SYSTEM_PROMPT)\n",
    "        .configure(json_mode=True)\n",
    "        .complete(INPUT)\n",
    "    )\n",
    "    result = json.loads(response)\n",
    "    keywords = {\"llm\": result[\"LLM\"], \"dev\": result[\"DEV\"]}\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def save_story_keywords(\n",
    "        output_file: Path, story_id: str, keywords: dict[str, str]\n",
    ") -> None:\n",
    "    \"\"\"Save keywords for a single story to CSV file\"\"\"\n",
    "    llm_keywords_str = json.dumps(keywords[\"llm\"]) if keywords[\"llm\"] else \"[]\"\n",
    "    dev_keywords_str = json.dumps(keywords[\"dev\"]) if keywords[\"dev\"] else \"[]\"\n",
    "\n",
    "    with open(output_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([story_id, llm_keywords_str, dev_keywords_str])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Fine-Tuning\n",
    "\n",
    "Test the keyword extraction on a random story sample to validate the LLM prompt and output quality before batch processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "random_story = df.sample(n=1).iloc[0]\n",
    "keywords = extract_keywords(random_story[\"title\"], random_story[\"story_text\"])\n",
    "\n",
    "print(f\"ID: {random_story['story_id']}\")\n",
    "print(f\"Title: {random_story['title']}\")\n",
    "print(f\"Content: {random_story['story_text']}\")\n",
    "print(\"\\nExtracted Keywords:\")\n",
    "print(f\"LLM Keywords: {keywords['llm']}\")\n",
    "print(f\"DEV Keywords: {keywords['dev']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "Execute the main pipeline with resume capability to extract keywords from all stories and save results incrementally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "initialize_output_file(output_file)\n",
    "processed_story_ids = get_processed_story_ids(output_file)\n",
    "\n",
    "total_stories = len(df)\n",
    "processed_count = len(processed_story_ids)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    story_id = row[\"story_id\"]\n",
    "\n",
    "    # Skip if already processed\n",
    "    if story_id in processed_story_ids:\n",
    "        continue\n",
    "\n",
    "    print(f\"üîç Processing story {processed_count + 1}/{total_stories}: {story_id}\")\n",
    "\n",
    "    if process_single_story(row, output_file):\n",
    "        processed_count += 1\n",
    "\n",
    "print(f\"\\nüéâ Processing complete!\")\n",
    "print(f\"Total stories processed: {processed_count}\")\n",
    "print(f\"Output saved to: {output_file}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
