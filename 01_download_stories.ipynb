{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40691ec31c33829",
   "metadata": {},
   "source": [
    "# Hacker News Story Downloader\n",
    "\n",
    "This notebook downloads Hacker News stories related to LLMs and AI coding tools by searching for predefined keywords (like “openai”, “chatgpt”, “copilot”, etc.) using the Hacker News API by Algolia. It systematically fetches all available story pages for each keyword, handles API errors with retry logic, and saves the results as separate JSON files in a data directory. The tool is designed to be polite with rate limiting and skips keywords that have already been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959a624c3d6e85c",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "The cell below defines keywords related to LLMs and Copilot apps that will be used to search and download relevant Hacker News stories. The approach could introduce duplicates, so we will perform deduplication later."
   ]
  },
  {
   "cell_type": "code",
   "id": "368bf87a865a0b7f",
   "metadata": {},
   "source": [
    "KEYWORDS_TO_SEARCH = [\n",
    "    \"openai\",\n",
    "    \"chatgpt\",\n",
    "    \"gpt\",\n",
    "    \"gemini\",\n",
    "    \"anthropic\",\n",
    "    \"claude\",\n",
    "    \"deepseek\",\n",
    "    \"grok\",\n",
    "    \"llama\",\n",
    "    \"mistral\",\n",
    "    \"copilot\",\n",
    "    \"cursor\",\n",
    "    \"cline\",\n",
    "    \"tabnine\",\n",
    "    \"JetBrains AI\",\n",
    "    \"Codeium\",\n",
    "    \"Windsurf\",\n",
    "    \"aider\",\n",
    "    \"zed\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58d8cd4a9a892bf",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "id": "e568b9bc",
   "metadata": {},
   "source": [
    "BASE_URL = \"http://hn.algolia.com/api/v1/search_by_date\"\n",
    "\n",
    "HITS_PER_PAGE = 50  # Number of results per API request page\n",
    "TAGS = \"story\"  # We are interested in stories\n",
    "NUMERIC_FILTERS = \"num_comments>0\"  # Only stories with comments\n",
    "\n",
    "REQUEST_DELAY_SECONDS = 1  # Politeness delay between successful page fetches\n",
    "REQUEST_TIMEOUT_SECONDS = 5  # Timeout for each API request attempt\n",
    "\n",
    "MAX_FETCH_RETRIES = 3  # Max attempts for a single page fetch\n",
    "RETRY_DELAY_SECONDS = 5  # Delay between retries for a failed page fetch\n",
    "\n",
    "OUTPUT_DIR = \"data/stories\"  # Directory to save the downloaded files"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c742376271bad113",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "This section defines all helper functions needed for the story download workflow, including API fetching with retry logic, file I/O operations, and orchestration functions that coordinate the entire process from keyword processing to JSON file creation."
   ]
  },
  {
   "cell_type": "code",
   "id": "9934bc526b27dfef",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_json_filepath(keyword: str) -> str:\n",
    "    \"\"\"Get the JSON filepath for a given keyword.\"\"\"\n",
    "    return os.path.join(OUTPUT_DIR, f\"{keyword}.json\")\n",
    "\n",
    "\n",
    "def process_single_keyword(keyword: str) -> bool:\n",
    "    \"\"\"Process a single keyword: check skip conditions, fetch stories, and save.\"\"\"\n",
    "    print(f'\\n--- Processing keyword: \"{keyword}\" ---')\n",
    "\n",
    "    if should_skip_keyword(keyword):\n",
    "        return True\n",
    "\n",
    "    all_stories = fetch_all_stories_for_keyword(keyword)\n",
    "    save_stories_to_json(keyword, all_stories)\n",
    "\n",
    "    print(f'--- Finished processing keyword: \"{keyword}\" ---')\n",
    "    return True\n",
    "\n",
    "\n",
    "def should_skip_keyword(keyword: str) -> bool:\n",
    "    \"\"\"Check if keyword should be skipped (file already exists).\"\"\"\n",
    "    json_filepath = get_json_filepath(keyword)\n",
    "    if os.path.exists(json_filepath):\n",
    "        print(\n",
    "            f'  JSON file for \"{keyword}\" already exists at {json_filepath}. Skipping.'\n",
    "        )\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fetch_all_stories_for_keyword(keyword: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch all pages of stories for a given keyword.\"\"\"\n",
    "    all_hits: List[Dict[str, Any]] = []\n",
    "    current_page = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            page_data = fetch_page_data(keyword, current_page)\n",
    "            if not page_data:\n",
    "                break\n",
    "\n",
    "            hits = page_data.get(\"hits\", [])\n",
    "            if not hits:\n",
    "                break\n",
    "\n",
    "            all_hits.extend(hits)\n",
    "            current_page += 1\n",
    "            time.sleep(REQUEST_DELAY_SECONDS)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"  Warning: Error occurred while fetching stories: {type(e).__name__} - {e}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    return all_hits\n",
    "\n",
    "\n",
    "def fetch_page_data(keyword: str, page_num: int) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches a single page of search results for a given keyword with retry logic.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": keyword,\n",
    "        \"tags\": TAGS,\n",
    "        \"numericFilters\": NUMERIC_FILTERS,\n",
    "        \"hitsPerPage\": HITS_PER_PAGE,\n",
    "        \"page\": page_num,\n",
    "    }\n",
    "\n",
    "    for attempt in range(MAX_FETCH_RETRIES):\n",
    "        print(\n",
    "            f'  Fetching page {page_num + 1} for \"{keyword}\" (Attempt {attempt + 1}/{MAX_FETCH_RETRIES})...'\n",
    "        )\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                BASE_URL, params=params, timeout=REQUEST_TIMEOUT_SECONDS\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"    Attempt {attempt + 1} FAILED: {type(e).__name__} - {e}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"    Attempt {attempt + 1} FAILED: {type(e).__name__} - {e}\")\n",
    "            if attempt < MAX_FETCH_RETRIES - 1:\n",
    "                print(f\"    Retrying in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(\n",
    "                    f'    All {MAX_FETCH_RETRIES} retries failed for page {page_num + 1} of keyword \"{keyword}\".'\n",
    "                )\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_stories_to_json(keyword: str, stories: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Saves a list of story dictionaries to a single JSON file for the given keyword.\n",
    "    \"\"\"\n",
    "    if not stories:\n",
    "        print(f'  No stories were accumulated for keyword \"{keyword}\" to save.')\n",
    "        json_filepath = get_json_filepath(keyword)\n",
    "        if not os.path.exists(json_filepath):\n",
    "            print(\n",
    "                f'  JSON file for \"{keyword}\" will not be created as no data was fetched successfully.'\n",
    "            )\n",
    "        return\n",
    "\n",
    "    json_filepath = get_json_filepath(keyword)\n",
    "\n",
    "    try:\n",
    "        with open(json_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(stories, f, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f'SUCCESS: Saved {len(stories)} stories for \"{keyword}\" to {json_filepath}'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f'ERROR: An unexpected error occurred while saving JSON for \"{keyword}\". Error: {e}'\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb9929864922c9e2",
   "metadata": {},
   "source": [
    "## Main Processing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f9fba74c34edf",
   "metadata": {},
   "source": [
    "Iterate through each keyword, fetch all pages of stories, and save them to a JSON file specific to that keyword."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dfa539ae3b70820",
   "metadata": {},
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"--- Starting Hacker News Story Downloader ---\")\n",
    "total_keywords_processed = 0\n",
    "\n",
    "for keyword in KEYWORDS_TO_SEARCH:\n",
    "    if process_single_keyword(keyword):\n",
    "        total_keywords_processed += 1\n",
    "\n",
    "print(\n",
    "    f\"\\n--- All {total_keywords_processed}/{len(KEYWORDS_TO_SEARCH)} keywords processed. \"\n",
    "    f\"Check the '{OUTPUT_DIR}' directory for JSON files. ---\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
